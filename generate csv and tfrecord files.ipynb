{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJsw7Ci_GwK9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'batu':\n",
        "        return 1\n",
        "    elif row_label == 'butir gabah':\n",
        "        return 2\n",
        "    elif row_label == 'butir kapur':\n",
        "        return 3\n",
        "    elif row_label == 'butir kepala':\n",
        "        return 4\n",
        "    elif row_label == 'butir menir':\n",
        "        return 5\n",
        "    elif row_label == 'butir merah':\n",
        "        return 6\n",
        "    elif row_label == 'butir patah':\n",
        "        return 7\n",
        "    elif row_label == 'butir rusak':\n",
        "        return 8\n",
        "    elif row_label == 'kutu':\n",
        "        return 9\n",
        "    elif row_label == 'sekam':\n",
        "        return 10\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "4xCMOQICIXYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIxfU__JGrSS",
        "outputId": "4f09eb10-6407-4271-b396-144e39bdc519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Dataset/dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7Q7BTG0G1j7",
        "outputId": "f316aec6-ca9b-454b-a2a3-248a4b801b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Dataset/dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/test/\n",
            "  inflating: dataset/test/IMG_0680.jpg  \n",
            "  inflating: dataset/test/IMG_0680.xml  \n",
            "  inflating: dataset/test/IMG_0681.jpg  \n",
            "  inflating: dataset/test/IMG_0681.xml  \n",
            "  inflating: dataset/test/IMG_0682.jpg  \n",
            "  inflating: dataset/test/IMG_0682.xml  \n",
            "  inflating: dataset/test/IMG_0683.jpg  \n",
            "  inflating: dataset/test/IMG_0683.xml  \n",
            "  inflating: dataset/test/IMG_0684.jpg  \n",
            "  inflating: dataset/test/IMG_0684.xml  \n",
            "  inflating: dataset/test/IMG_0685.jpg  \n",
            "  inflating: dataset/test/IMG_0685.xml  \n",
            "  inflating: dataset/test/IMG_0687.jpg  \n",
            "  inflating: dataset/test/IMG_0687.xml  \n",
            "  inflating: dataset/test/IMG_0688.jpg  \n",
            "  inflating: dataset/test/IMG_0688.xml  \n",
            "  inflating: dataset/test/IMG_0689.jpg  \n",
            "  inflating: dataset/test/IMG_0689.xml  \n",
            "  inflating: dataset/test/IMG_0690.jpg  \n",
            "  inflating: dataset/test/IMG_0690.xml  \n",
            "  inflating: dataset/test/IMG_0691.jpg  \n",
            "  inflating: dataset/test/IMG_0691.xml  \n",
            "  inflating: dataset/test/IMG_0692.jpg  \n",
            "  inflating: dataset/test/IMG_0692.xml  \n",
            "  inflating: dataset/test/IMG_0693.jpg  \n",
            "  inflating: dataset/test/IMG_0693.xml  \n",
            "  inflating: dataset/test/IMG_0694.jpg  \n",
            "  inflating: dataset/test/IMG_0694.xml  \n",
            "  inflating: dataset/test/IMG_0695.jpg  \n",
            "  inflating: dataset/test/IMG_0695.xml  \n",
            "  inflating: dataset/test/IMG_0696.jpg  \n",
            "  inflating: dataset/test/IMG_0696.xml  \n",
            "  inflating: dataset/test/IMG_0697.jpg  \n",
            "  inflating: dataset/test/IMG_0697.xml  \n",
            "  inflating: dataset/test/IMG_0698.jpg  \n",
            "  inflating: dataset/test/IMG_0698.xml  \n",
            "  inflating: dataset/test/IMG_0699.jpg  \n",
            "  inflating: dataset/test/IMG_0699.xml  \n",
            "  inflating: dataset/test/IMG_0700.jpg  \n",
            "  inflating: dataset/test/IMG_0700.xml  \n",
            "  inflating: dataset/test/IMG_0701.jpg  \n",
            "  inflating: dataset/test/IMG_0701.xml  \n",
            "  inflating: dataset/test/IMG_0702.jpg  \n",
            "  inflating: dataset/test/IMG_0702.xml  \n",
            "  inflating: dataset/test/IMG_0703.jpg  \n",
            "  inflating: dataset/test/IMG_0703.xml  \n",
            "  inflating: dataset/test/IMG_0704.jpg  \n",
            "  inflating: dataset/test/IMG_0704.xml  \n",
            "  inflating: dataset/test/IMG_0705.jpg  \n",
            "  inflating: dataset/test/IMG_0705.xml  \n",
            "  inflating: dataset/test/IMG_0706.jpg  \n",
            "  inflating: dataset/test/IMG_0706.xml  \n",
            "  inflating: dataset/test/IMG_0707.jpg  \n",
            "  inflating: dataset/test/IMG_0707.xml  \n",
            "  inflating: dataset/test/IMG_0708.jpg  \n",
            "  inflating: dataset/test/IMG_0708.xml  \n",
            "  inflating: dataset/test/IMG_0709.jpg  \n",
            "  inflating: dataset/test/IMG_0709.xml  \n",
            "  inflating: dataset/test/IMG_0710.jpg  \n",
            "  inflating: dataset/test/IMG_0710.xml  \n",
            "  inflating: dataset/test/IMG_0711.jpg  \n",
            "  inflating: dataset/test/IMG_0711.xml  \n",
            "  inflating: dataset/test/IMG_0712.jpg  \n",
            "  inflating: dataset/test/IMG_0712.xml  \n",
            "  inflating: dataset/test/IMG_0713.jpg  \n",
            "  inflating: dataset/test/IMG_0713.xml  \n",
            "  inflating: dataset/test/IMG_0714.jpg  \n",
            "  inflating: dataset/test/IMG_0714.xml  \n",
            "  inflating: dataset/test/IMG_0715.jpg  \n",
            "  inflating: dataset/test/IMG_0715.xml  \n",
            "  inflating: dataset/test/IMG_0716.jpg  \n",
            "  inflating: dataset/test/IMG_0716.xml  \n",
            "  inflating: dataset/test/IMG_0717.jpg  \n",
            "  inflating: dataset/test/IMG_0717.xml  \n",
            "  inflating: dataset/test/IMG_0719.jpg  \n",
            "  inflating: dataset/test/IMG_0719.xml  \n",
            "  inflating: dataset/test/IMG_0720.jpg  \n",
            "  inflating: dataset/test/IMG_0720.xml  \n",
            "  inflating: dataset/test/IMG_0721.jpg  \n",
            "  inflating: dataset/test/IMG_0721.xml  \n",
            "  inflating: dataset/test/IMG_0722.jpg  \n",
            "  inflating: dataset/test/IMG_0722.xml  \n",
            "  inflating: dataset/test/IMG_0723.jpg  \n",
            "  inflating: dataset/test/IMG_0723.xml  \n",
            "  inflating: dataset/test/IMG_0724.jpg  \n",
            "  inflating: dataset/test/IMG_0724.xml  \n",
            "  inflating: dataset/test/IMG_0725.jpg  \n",
            "  inflating: dataset/test/IMG_0725.xml  \n",
            "  inflating: dataset/test/IMG_0726.jpg  \n",
            "  inflating: dataset/test/IMG_0726.xml  \n",
            "  inflating: dataset/test/IMG_0736.jpg  \n",
            "  inflating: dataset/test/IMG_0736.xml  \n",
            "  inflating: dataset/test/IMG_0737.jpg  \n",
            "  inflating: dataset/test/IMG_0737.xml  \n",
            "  inflating: dataset/test/IMG_20230916_080402.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_080402.xml  \n",
            "  inflating: dataset/test/IMG_20230916_080704.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_080704.xml  \n",
            "  inflating: dataset/test/IMG_20230916_080822.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_080822.xml  \n",
            "  inflating: dataset/test/IMG_20230916_080831.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_080831.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081049.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081049.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081221.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081221.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081342.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081342.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081505.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081505.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081605.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081605.xml  \n",
            "  inflating: dataset/test/IMG_20230916_081722.jpg  \n",
            "  inflating: dataset/test/IMG_20230916_081722.xml  \n",
            "   creating: dataset/train/\n",
            "  inflating: dataset/train/IMG_0478.jpg  \n",
            "  inflating: dataset/train/IMG_0478.xml  \n",
            "  inflating: dataset/train/IMG_0479.jpg  \n",
            "  inflating: dataset/train/IMG_0479.xml  \n",
            "  inflating: dataset/train/IMG_0480.jpg  \n",
            "  inflating: dataset/train/IMG_0480.xml  \n",
            "  inflating: dataset/train/IMG_0481.jpg  \n",
            "  inflating: dataset/train/IMG_0481.xml  \n",
            "  inflating: dataset/train/IMG_0482.jpg  \n",
            "  inflating: dataset/train/IMG_0482.xml  \n",
            "  inflating: dataset/train/IMG_0483.jpg  \n",
            "  inflating: dataset/train/IMG_0483.xml  \n",
            "  inflating: dataset/train/IMG_0484.jpg  \n",
            "  inflating: dataset/train/IMG_0484.xml  \n",
            "  inflating: dataset/train/IMG_0485.jpg  \n",
            "  inflating: dataset/train/IMG_0485.xml  \n",
            "  inflating: dataset/train/IMG_0486.jpg  \n",
            "  inflating: dataset/train/IMG_0486.xml  \n",
            "  inflating: dataset/train/IMG_0487.jpg  \n",
            "  inflating: dataset/train/IMG_0487.xml  \n",
            "  inflating: dataset/train/IMG_0489.jpg  \n",
            "  inflating: dataset/train/IMG_0489.xml  \n",
            "  inflating: dataset/train/IMG_0490.jpg  \n",
            "  inflating: dataset/train/IMG_0490.xml  \n",
            "  inflating: dataset/train/IMG_0491.jpg  \n",
            "  inflating: dataset/train/IMG_0491.xml  \n",
            "  inflating: dataset/train/IMG_0492.jpg  \n",
            "  inflating: dataset/train/IMG_0492.xml  \n",
            "  inflating: dataset/train/IMG_0493.jpg  \n",
            "  inflating: dataset/train/IMG_0493.xml  \n",
            "  inflating: dataset/train/IMG_0494.jpg  \n",
            "  inflating: dataset/train/IMG_0494.xml  \n",
            "  inflating: dataset/train/IMG_0495.jpg  \n",
            "  inflating: dataset/train/IMG_0495.xml  \n",
            "  inflating: dataset/train/IMG_0496.jpg  \n",
            "  inflating: dataset/train/IMG_0496.xml  \n",
            "  inflating: dataset/train/IMG_0497.jpg  \n",
            "  inflating: dataset/train/IMG_0497.xml  \n",
            "  inflating: dataset/train/IMG_0498.jpg  \n",
            "  inflating: dataset/train/IMG_0498.xml  \n",
            "  inflating: dataset/train/IMG_0499.jpg  \n",
            "  inflating: dataset/train/IMG_0499.xml  \n",
            "  inflating: dataset/train/IMG_0500.jpg  \n",
            "  inflating: dataset/train/IMG_0500.xml  \n",
            "  inflating: dataset/train/IMG_0502.jpg  \n",
            "  inflating: dataset/train/IMG_0502.xml  \n",
            "  inflating: dataset/train/IMG_0503.jpg  \n",
            "  inflating: dataset/train/IMG_0503.xml  \n",
            "  inflating: dataset/train/IMG_0504.jpg  \n",
            "  inflating: dataset/train/IMG_0504.xml  \n",
            "  inflating: dataset/train/IMG_0505.jpg  \n",
            "  inflating: dataset/train/IMG_0505.xml  \n",
            "  inflating: dataset/train/IMG_0506.jpg  \n",
            "  inflating: dataset/train/IMG_0506.xml  \n",
            "  inflating: dataset/train/IMG_0507.jpg  \n",
            "  inflating: dataset/train/IMG_0507.xml  \n",
            "  inflating: dataset/train/IMG_0508.jpg  \n",
            "  inflating: dataset/train/IMG_0508.xml  \n",
            "  inflating: dataset/train/IMG_0509.jpg  \n",
            "  inflating: dataset/train/IMG_0509.xml  \n",
            "  inflating: dataset/train/IMG_0510.jpg  \n",
            "  inflating: dataset/train/IMG_0510.xml  \n",
            "  inflating: dataset/train/IMG_0511.jpg  \n",
            "  inflating: dataset/train/IMG_0511.xml  \n",
            "  inflating: dataset/train/IMG_0512.jpg  \n",
            "  inflating: dataset/train/IMG_0512.xml  \n",
            "  inflating: dataset/train/IMG_0513.jpg  \n",
            "  inflating: dataset/train/IMG_0513.xml  \n",
            "  inflating: dataset/train/IMG_0514.jpg  \n",
            "  inflating: dataset/train/IMG_0514.xml  \n",
            "  inflating: dataset/train/IMG_0515.jpg  \n",
            "  inflating: dataset/train/IMG_0515.xml  \n",
            "  inflating: dataset/train/IMG_0516.jpg  \n",
            "  inflating: dataset/train/IMG_0516.xml  \n",
            "  inflating: dataset/train/IMG_0517.jpg  \n",
            "  inflating: dataset/train/IMG_0517.xml  \n",
            "  inflating: dataset/train/IMG_0518.jpg  \n",
            "  inflating: dataset/train/IMG_0518.xml  \n",
            "  inflating: dataset/train/IMG_0519.jpg  \n",
            "  inflating: dataset/train/IMG_0519.xml  \n",
            "  inflating: dataset/train/IMG_0521.jpg  \n",
            "  inflating: dataset/train/IMG_0521.xml  \n",
            "  inflating: dataset/train/IMG_0522.jpg  \n",
            "  inflating: dataset/train/IMG_0522.xml  \n",
            "  inflating: dataset/train/IMG_0523.jpg  \n",
            "  inflating: dataset/train/IMG_0523.xml  \n",
            "  inflating: dataset/train/IMG_0524.jpg  \n",
            "  inflating: dataset/train/IMG_0524.xml  \n",
            "  inflating: dataset/train/IMG_0525.jpg  \n",
            "  inflating: dataset/train/IMG_0525.xml  \n",
            "  inflating: dataset/train/IMG_0526.jpg  \n",
            "  inflating: dataset/train/IMG_0526.xml  \n",
            "  inflating: dataset/train/IMG_0527.jpg  \n",
            "  inflating: dataset/train/IMG_0527.xml  \n",
            "  inflating: dataset/train/IMG_0528.jpg  \n",
            "  inflating: dataset/train/IMG_0528.xml  \n",
            "  inflating: dataset/train/IMG_0529.jpg  \n",
            "  inflating: dataset/train/IMG_0529.xml  \n",
            "  inflating: dataset/train/IMG_0530.jpg  \n",
            "  inflating: dataset/train/IMG_0530.xml  \n",
            "  inflating: dataset/train/IMG_0531.jpg  \n",
            "  inflating: dataset/train/IMG_0531.xml  \n",
            "  inflating: dataset/train/IMG_0532.jpg  \n",
            "  inflating: dataset/train/IMG_0532.xml  \n",
            "  inflating: dataset/train/IMG_0533.jpg  \n",
            "  inflating: dataset/train/IMG_0533.xml  \n",
            "  inflating: dataset/train/IMG_0534.jpg  \n",
            "  inflating: dataset/train/IMG_0534.xml  \n",
            "  inflating: dataset/train/IMG_0535.jpg  \n",
            "  inflating: dataset/train/IMG_0535.xml  \n",
            "  inflating: dataset/train/IMG_0536.jpg  \n",
            "  inflating: dataset/train/IMG_0536.xml  \n",
            "  inflating: dataset/train/IMG_0537.jpg  \n",
            "  inflating: dataset/train/IMG_0537.xml  \n",
            "  inflating: dataset/train/IMG_0538.jpg  \n",
            "  inflating: dataset/train/IMG_0538.xml  \n",
            "  inflating: dataset/train/IMG_0539.jpg  \n",
            "  inflating: dataset/train/IMG_0539.xml  \n",
            "  inflating: dataset/train/IMG_0540.jpg  \n",
            "  inflating: dataset/train/IMG_0540.xml  \n",
            "  inflating: dataset/train/IMG_0541.jpg  \n",
            "  inflating: dataset/train/IMG_0541.xml  \n",
            "  inflating: dataset/train/IMG_0542.jpg  \n",
            "  inflating: dataset/train/IMG_0542.xml  \n",
            "  inflating: dataset/train/IMG_0543.jpg  \n",
            "  inflating: dataset/train/IMG_0543.xml  \n",
            "  inflating: dataset/train/IMG_0544.jpg  \n",
            "  inflating: dataset/train/IMG_0544.xml  \n",
            "  inflating: dataset/train/IMG_0545.jpg  \n",
            "  inflating: dataset/train/IMG_0545.xml  \n",
            "  inflating: dataset/train/IMG_0546.jpg  \n",
            "  inflating: dataset/train/IMG_0546.xml  \n",
            "  inflating: dataset/train/IMG_0547.jpg  \n",
            "  inflating: dataset/train/IMG_0547.xml  \n",
            "  inflating: dataset/train/IMG_0548.jpg  \n",
            "  inflating: dataset/train/IMG_0548.xml  \n",
            "  inflating: dataset/train/IMG_0549.jpg  \n",
            "  inflating: dataset/train/IMG_0549.xml  \n",
            "  inflating: dataset/train/IMG_0550.jpg  \n",
            "  inflating: dataset/train/IMG_0550.xml  \n",
            "  inflating: dataset/train/IMG_0551.jpg  \n",
            "  inflating: dataset/train/IMG_0551.xml  \n",
            "  inflating: dataset/train/IMG_0552.jpg  \n",
            "  inflating: dataset/train/IMG_0552.xml  \n",
            "  inflating: dataset/train/IMG_0553.jpg  \n",
            "  inflating: dataset/train/IMG_0553.xml  \n",
            "  inflating: dataset/train/IMG_0554.jpg  \n",
            "  inflating: dataset/train/IMG_0554.xml  \n",
            "  inflating: dataset/train/IMG_0555.jpg  \n",
            "  inflating: dataset/train/IMG_0555.xml  \n",
            "  inflating: dataset/train/IMG_0595.jpg  \n",
            "  inflating: dataset/train/IMG_0595.xml  \n",
            "  inflating: dataset/train/IMG_0597.jpg  \n",
            "  inflating: dataset/train/IMG_0597.xml  \n",
            "  inflating: dataset/train/IMG_0598.jpg  \n",
            "  inflating: dataset/train/IMG_0598.xml  \n",
            "  inflating: dataset/train/IMG_0599.jpg  \n",
            "  inflating: dataset/train/IMG_0599.xml  \n",
            "  inflating: dataset/train/IMG_0600.jpg  \n",
            "  inflating: dataset/train/IMG_0600.xml  \n",
            "  inflating: dataset/train/IMG_0601.jpg  \n",
            "  inflating: dataset/train/IMG_0601.xml  \n",
            "  inflating: dataset/train/IMG_0602.jpg  \n",
            "  inflating: dataset/train/IMG_0602.xml  \n",
            "  inflating: dataset/train/IMG_0603.jpg  \n",
            "  inflating: dataset/train/IMG_0603.xml  \n",
            "  inflating: dataset/train/IMG_0604.jpg  \n",
            "  inflating: dataset/train/IMG_0604.xml  \n",
            "  inflating: dataset/train/IMG_0605.jpg  \n",
            "  inflating: dataset/train/IMG_0605.xml  \n",
            "  inflating: dataset/train/IMG_0606.jpg  \n",
            "  inflating: dataset/train/IMG_0606.xml  \n",
            "  inflating: dataset/train/IMG_0607.jpg  \n",
            "  inflating: dataset/train/IMG_0607.xml  \n",
            "  inflating: dataset/train/IMG_0608.jpg  \n",
            "  inflating: dataset/train/IMG_0608.xml  \n",
            "  inflating: dataset/train/IMG_0609.jpg  \n",
            "  inflating: dataset/train/IMG_0609.xml  \n",
            "  inflating: dataset/train/IMG_0610.jpg  \n",
            "  inflating: dataset/train/IMG_0610.xml  \n",
            "  inflating: dataset/train/IMG_0611.jpg  \n",
            "  inflating: dataset/train/IMG_0611.xml  \n",
            "  inflating: dataset/train/IMG_0612.jpg  \n",
            "  inflating: dataset/train/IMG_0612.xml  \n",
            "  inflating: dataset/train/IMG_0613.jpg  \n",
            "  inflating: dataset/train/IMG_0613.xml  \n",
            "  inflating: dataset/train/IMG_0614.jpg  \n",
            "  inflating: dataset/train/IMG_0614.xml  \n",
            "  inflating: dataset/train/IMG_0615.jpg  \n",
            "  inflating: dataset/train/IMG_0615.xml  \n",
            "  inflating: dataset/train/IMG_0616.jpg  \n",
            "  inflating: dataset/train/IMG_0616.xml  \n",
            "  inflating: dataset/train/IMG_0617.jpg  \n",
            "  inflating: dataset/train/IMG_0617.xml  \n",
            "  inflating: dataset/train/IMG_0618.jpg  \n",
            "  inflating: dataset/train/IMG_0618.xml  \n",
            "  inflating: dataset/train/IMG_0619.jpg  \n",
            "  inflating: dataset/train/IMG_0619.xml  \n",
            "  inflating: dataset/train/IMG_0620.jpg  \n",
            "  inflating: dataset/train/IMG_0620.xml  \n",
            "  inflating: dataset/train/IMG_0621.jpg  \n",
            "  inflating: dataset/train/IMG_0621.xml  \n",
            "  inflating: dataset/train/IMG_0622.jpg  \n",
            "  inflating: dataset/train/IMG_0622.xml  \n",
            "  inflating: dataset/train/IMG_0623.jpg  \n",
            "  inflating: dataset/train/IMG_0623.xml  \n",
            "  inflating: dataset/train/IMG_0624.jpg  \n",
            "  inflating: dataset/train/IMG_0624.xml  \n",
            "  inflating: dataset/train/IMG_0625.jpg  \n",
            "  inflating: dataset/train/IMG_0625.xml  \n",
            "  inflating: dataset/train/IMG_0626.jpg  \n",
            "  inflating: dataset/train/IMG_0626.xml  \n",
            "  inflating: dataset/train/IMG_0627.jpg  \n",
            "  inflating: dataset/train/IMG_0627.xml  \n",
            "  inflating: dataset/train/IMG_0628.jpg  \n",
            "  inflating: dataset/train/IMG_0628.xml  \n",
            "  inflating: dataset/train/IMG_0629.jpg  \n",
            "  inflating: dataset/train/IMG_0629.xml  \n",
            "  inflating: dataset/train/IMG_0630.jpg  \n",
            "  inflating: dataset/train/IMG_0630.xml  \n",
            "  inflating: dataset/train/IMG_0631.jpg  \n",
            "  inflating: dataset/train/IMG_0631.xml  \n",
            "  inflating: dataset/train/IMG_0632.jpg  \n",
            "  inflating: dataset/train/IMG_0632.xml  \n",
            "  inflating: dataset/train/IMG_0633.jpg  \n",
            "  inflating: dataset/train/IMG_0633.xml  \n",
            "  inflating: dataset/train/IMG_0634.jpg  \n",
            "  inflating: dataset/train/IMG_0634.xml  \n",
            "  inflating: dataset/train/IMG_0635.jpg  \n",
            "  inflating: dataset/train/IMG_0635.xml  \n",
            "  inflating: dataset/train/IMG_0636.jpg  \n",
            "  inflating: dataset/train/IMG_0636.xml  \n",
            "  inflating: dataset/train/IMG_0637.jpg  \n",
            "  inflating: dataset/train/IMG_0637.xml  \n",
            "  inflating: dataset/train/IMG_0638.jpg  \n",
            "  inflating: dataset/train/IMG_0638.xml  \n",
            "  inflating: dataset/train/IMG_0639.jpg  \n",
            "  inflating: dataset/train/IMG_0639.xml  \n",
            "  inflating: dataset/train/IMG_0640.jpg  \n",
            "  inflating: dataset/train/IMG_0640.xml  \n",
            "  inflating: dataset/train/IMG_0641.jpg  \n",
            "  inflating: dataset/train/IMG_0641.xml  \n",
            "  inflating: dataset/train/IMG_0642.jpg  \n",
            "  inflating: dataset/train/IMG_0642.xml  \n",
            "  inflating: dataset/train/IMG_0643.jpg  \n",
            "  inflating: dataset/train/IMG_0643.xml  \n",
            "  inflating: dataset/train/IMG_0644.jpg  \n",
            "  inflating: dataset/train/IMG_0644.xml  \n",
            "  inflating: dataset/train/IMG_0645.jpg  \n",
            "  inflating: dataset/train/IMG_0645.xml  \n",
            "  inflating: dataset/train/IMG_0646.jpg  \n",
            "  inflating: dataset/train/IMG_0646.xml  \n",
            "  inflating: dataset/train/IMG_0647.jpg  \n",
            "  inflating: dataset/train/IMG_0647.xml  \n",
            "  inflating: dataset/train/IMG_0648.jpg  \n",
            "  inflating: dataset/train/IMG_0648.xml  \n",
            "  inflating: dataset/train/IMG_0649.jpg  \n",
            "  inflating: dataset/train/IMG_0649.xml  \n",
            "  inflating: dataset/train/IMG_0650.jpg  \n",
            "  inflating: dataset/train/IMG_0650.xml  \n",
            "  inflating: dataset/train/IMG_0651.jpg  \n",
            "  inflating: dataset/train/IMG_0651.xml  \n",
            "  inflating: dataset/train/IMG_0652.jpg  \n",
            "  inflating: dataset/train/IMG_0652.xml  \n",
            "  inflating: dataset/train/IMG_0653.jpg  \n",
            "  inflating: dataset/train/IMG_0653.xml  \n",
            "  inflating: dataset/train/IMG_0654.jpg  \n",
            "  inflating: dataset/train/IMG_0654.xml  \n",
            "  inflating: dataset/train/IMG_0655.jpg  \n",
            "  inflating: dataset/train/IMG_0655.xml  \n",
            "  inflating: dataset/train/IMG_0656.jpg  \n",
            "  inflating: dataset/train/IMG_0656.xml  \n",
            "  inflating: dataset/train/IMG_0657.jpg  \n",
            "  inflating: dataset/train/IMG_0657.xml  \n",
            "  inflating: dataset/train/IMG_0658.jpg  \n",
            "  inflating: dataset/train/IMG_0658.xml  \n",
            "  inflating: dataset/train/IMG_0659.jpg  \n",
            "  inflating: dataset/train/IMG_0659.xml  \n",
            "  inflating: dataset/train/IMG_0660.jpg  \n",
            "  inflating: dataset/train/IMG_0660.xml  \n",
            "  inflating: dataset/train/IMG_0661.jpg  \n",
            "  inflating: dataset/train/IMG_0661.xml  \n",
            "  inflating: dataset/train/IMG_0662.jpg  \n",
            "  inflating: dataset/train/IMG_0662.xml  \n",
            "  inflating: dataset/train/IMG_0663.jpg  \n",
            "  inflating: dataset/train/IMG_0663.xml  \n",
            "  inflating: dataset/train/IMG_0664.jpg  \n",
            "  inflating: dataset/train/IMG_0664.xml  \n",
            "  inflating: dataset/train/IMG_0665.jpg  \n",
            "  inflating: dataset/train/IMG_0665.xml  \n",
            "  inflating: dataset/train/IMG_0666.jpg  \n",
            "  inflating: dataset/train/IMG_0666.xml  \n",
            "  inflating: dataset/train/IMG_0667.jpg  \n",
            "  inflating: dataset/train/IMG_0667.xml  \n",
            "  inflating: dataset/train/IMG_0668.jpg  \n",
            "  inflating: dataset/train/IMG_0668.xml  \n",
            "  inflating: dataset/train/IMG_0669.jpg  \n",
            "  inflating: dataset/train/IMG_0669.xml  \n",
            "  inflating: dataset/train/IMG_0670.jpg  \n",
            "  inflating: dataset/train/IMG_0670.xml  \n",
            "  inflating: dataset/train/IMG_0671.jpg  \n",
            "  inflating: dataset/train/IMG_0671.xml  \n",
            "  inflating: dataset/train/IMG_0672.jpg  \n",
            "  inflating: dataset/train/IMG_0672.xml  \n",
            "  inflating: dataset/train/IMG_0673.jpg  \n",
            "  inflating: dataset/train/IMG_0673.xml  \n",
            "  inflating: dataset/train/IMG_0674.jpg  \n",
            "  inflating: dataset/train/IMG_0674.xml  \n",
            "  inflating: dataset/train/IMG_0675.jpg  \n",
            "  inflating: dataset/train/IMG_0675.xml  \n",
            "  inflating: dataset/train/IMG_0676.jpg  \n",
            "  inflating: dataset/train/IMG_0676.xml  \n",
            "  inflating: dataset/train/IMG_0677.jpg  \n",
            "  inflating: dataset/train/IMG_0677.xml  \n",
            "  inflating: dataset/train/IMG_0678.jpg  \n",
            "  inflating: dataset/train/IMG_0678.xml  \n",
            "  inflating: dataset/train/IMG_0679.jpg  \n",
            "  inflating: dataset/train/IMG_0679.xml  \n",
            "  inflating: dataset/train/IMG_0727.jpg  \n",
            "  inflating: dataset/train/IMG_0727.xml  \n",
            "  inflating: dataset/train/IMG_0728.jpg  \n",
            "  inflating: dataset/train/IMG_0728.xml  \n",
            "  inflating: dataset/train/IMG_0729.jpg  \n",
            "  inflating: dataset/train/IMG_0729.xml  \n",
            "  inflating: dataset/train/IMG_0730.jpg  \n",
            "  inflating: dataset/train/IMG_0730.xml  \n",
            "  inflating: dataset/train/IMG_0731.jpg  \n",
            "  inflating: dataset/train/IMG_0731.xml  \n",
            "  inflating: dataset/train/IMG_0732.jpg  \n",
            "  inflating: dataset/train/IMG_0732.xml  \n",
            "  inflating: dataset/train/IMG_0733.jpg  \n",
            "  inflating: dataset/train/IMG_0733.xml  \n",
            "  inflating: dataset/train/IMG_0734.jpg  \n",
            "  inflating: dataset/train/IMG_0734.xml  \n",
            "  inflating: dataset/train/IMG_0735.jpg  \n",
            "  inflating: dataset/train/IMG_0735.xml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_csv(xml_folder, csv_output):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(os.path.join(xml_folder, '*.xml')):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text))\n",
        "            xml_list.append(value)\n",
        "\n",
        "    column_names = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_names)\n",
        "    xml_df.to_csv(csv_output, index=None)\n",
        "    print(f'CSV file saved to {csv_output}')\n",
        "\n",
        "# Contoh penggunaan:\n",
        "xml_folder = '/content/dataset/train'\n",
        "csv_output = 'train.csv'\n",
        "xml_to_csv(xml_folder, csv_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwdXqg2IMOp6",
        "outputId": "826698c6-c6dd-43a5-dea7-604a3dc9dfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_csv_test(xml_folder, csv_output):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(os.path.join(xml_folder, '*.xml')):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text))\n",
        "            xml_list.append(value)\n",
        "\n",
        "    column_names = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_names)\n",
        "    xml_df.to_csv(csv_output, index=None)\n",
        "    print(f'CSV file saved to {csv_output}')\n",
        "\n",
        "# Contoh penggunaan:\n",
        "xml_folder = '/content/dataset/test'\n",
        "csv_output = 'test.csv'\n",
        "xml_to_csv(xml_folder, csv_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QPPGDdKMoys",
        "outputId": "3e1dc6d0-d19a-43a5-e59f-3683cbcf402d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved to test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "def resize_image(image_path, output_path, target_size=(640, 640)):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Faktor Resize\n",
        "    resize_factor = min(target_size[0] / image.shape[1], target_size[1] / image.shape[0])\n",
        "\n",
        "    # Resize gambar\n",
        "    resized_image = cv2.resize(image, None, fx=resize_factor, fy=resize_factor)\n",
        "\n",
        "    # Simpan gambar yang sudah diresize\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "    return resized_image, resize_factor\n",
        "\n",
        "def update_csv(csv_path, resize_factor):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Hitung ulang koordinat bounding box\n",
        "        df.at[index, 'xmin'] = int(row['xmin'] * resize_factor)\n",
        "        df.at[index, 'ymin'] = int(row['ymin'] * resize_factor)\n",
        "        df.at[index, 'xmax'] = int(row['xmax'] * resize_factor)\n",
        "        df.at[index, 'ymax'] = int(row['ymax'] * resize_factor)\n",
        "        df.at[index, 'width'] = int(row['width'] * resize_factor)\n",
        "        df.at[index, 'height'] = int(row['height'] * resize_factor)\n",
        "\n",
        "    # Menyimpan file CSV yang telah diperbarui sebagai file baru\n",
        "    new_csv_path = csv_path.replace('.csv', '_resized_640.csv')\n",
        "    df.to_csv(new_csv_path, index=False)\n",
        "    return new_csv_path\n",
        "\n",
        "def resize_images_in_folder(input_folder, output_folder, target_size=(640, 640)):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Sesuaikan dengan jenis gambar yang digunakan\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Resize gambar dan perbarui CSV\n",
        "            resized_image, resize_factor = resize_image(input_path, output_path, target_size)\n",
        "\n",
        "    # Perbarui file CSV dengan koordinat yang sesuai\n",
        "    dt_input_folder = '/content'\n",
        "    csv_path = os.path.join(dt_input_folder, 'train.csv')  # Sesuaikan dengan nama file CSV Anda\n",
        "    new_csv_path = update_csv(csv_path, resize_factor)\n",
        "\n",
        "    return new_csv_path\n",
        "\n",
        "# Contoh pemanggilan fungsi\n",
        "input_folder_path = '/content/dataset/train'\n",
        "output_folder_path = '/content/dataset/train_resize_640'\n",
        "new_csv_path = resize_images_in_folder(input_folder_path, output_folder_path)\n",
        "\n",
        "print(f\"File CSV yang diperbarui disimpan sebagai: {new_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRLxHJANygPX",
        "outputId": "e23ad009-eb28-4a29-9404-47abce6134f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File CSV yang diperbarui disimpan sebagai: /content/train_resized_640.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "def resize_image(image_path, output_path, target_size=(640, 640)):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Faktor Resize\n",
        "    resize_factor = min(target_size[0] / image.shape[1], target_size[1] / image.shape[0])\n",
        "\n",
        "    # Resize gambar\n",
        "    resized_image = cv2.resize(image, None, fx=resize_factor, fy=resize_factor)\n",
        "\n",
        "    # Simpan gambar yang sudah diresize\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "    return resized_image, resize_factor\n",
        "\n",
        "def update_csv(csv_path, resize_factor):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Hitung ulang koordinat bounding box\n",
        "        df.at[index, 'xmin'] = int(row['xmin'] * resize_factor)\n",
        "        df.at[index, 'ymin'] = int(row['ymin'] * resize_factor)\n",
        "        df.at[index, 'xmax'] = int(row['xmax'] * resize_factor)\n",
        "        df.at[index, 'ymax'] = int(row['ymax'] * resize_factor)\n",
        "        df.at[index, 'width'] = int(row['width'] * resize_factor)\n",
        "        df.at[index, 'height'] = int(row['height'] * resize_factor)\n",
        "\n",
        "    # Menyimpan file CSV yang telah diperbarui sebagai file baru\n",
        "    new_csv_path = csv_path.replace('.csv', '_resized_640.csv')\n",
        "    df.to_csv(new_csv_path, index=False)\n",
        "    return new_csv_path\n",
        "\n",
        "def resize_images_in_folder(input_folder, output_folder, target_size=(640, 640)):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Sesuaikan dengan jenis gambar yang digunakan\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Resize gambar dan perbarui CSV\n",
        "            resized_image, resize_factor = resize_image(input_path, output_path, target_size)\n",
        "\n",
        "    # Perbarui file CSV dengan koordinat yang sesuai\n",
        "    dt_input_folder = '/content'\n",
        "    csv_path = os.path.join(dt_input_folder, 'test.csv')  # Sesuaikan dengan nama file CSV Anda\n",
        "    new_csv_path = update_csv(csv_path, resize_factor)\n",
        "\n",
        "    return new_csv_path\n",
        "\n",
        "# Contoh pemanggilan fungsi\n",
        "input_folder_path = '/content/dataset/test'\n",
        "output_folder_path = '/content/dataset/test_resize_640'\n",
        "new_csv_path = resize_images_in_folder(input_folder_path, output_folder_path)\n",
        "\n",
        "print(f\"File CSV yang diperbarui disimpan sebagai: {new_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvq0AE7c4xPi",
        "outputId": "273cb78f-5113-46db-da4c-0b4038f81301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File CSV yang diperbarui disimpan sebagai: /content/test_resized_640.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/dataset/train_resize_640'\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "# Fungsi konversi tipe data untuk fitur numerik\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(value)]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "# Update your create_tf_example function to use the original image\n",
        "def create_tf_example(row):\n",
        "    # Mengonversi gambar menjadi bytes\n",
        "    image_path = os.path.join(image_dir, row['filename'])\n",
        "\n",
        "    with tf.io.gfile.GFile(image_path, 'rb') as fid:\n",
        "        encoded_image = fid.read()\n",
        "\n",
        "    feature = {\n",
        "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image])),\n",
        "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b'jpeg'])),\n",
        "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=[row['xmin']])),\n",
        "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=[row['xmax']])),\n",
        "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=[row['ymin']])),\n",
        "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=[row['ymax']])),\n",
        "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=[class_text_to_int(row['class'])])),\n",
        "    }\n",
        "\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example\n",
        "\n",
        "csv_file_path = '/content/train_resized_640.csv'\n",
        "\n",
        "# Membaca file CSV\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Gantilah dengan path untuk menyimpan file TFRecord\n",
        "output_tfrecord_path = '/content/training.tfrecord'\n",
        "\n",
        "# Menulis data ke dalam file TFRecord\n",
        "with tf.io.TFRecordWriter(output_tfrecord_path) as writer:\n",
        "    for index, row in df.iterrows():\n",
        "        tf_example = create_tf_example(row)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "print(f\"TFRecord file created at: {output_tfrecord_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JozriyipIe2m",
        "outputId": "9d8b214a-039c-4b0d-e8c2-211a705298cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecord file created at: /content/training.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/dataset/test_resize_640'\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "# Fungsi konversi tipe data untuk fitur numerik\n",
        "def _int64_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(value)]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "# Update your create_tf_example function to use the original image\n",
        "def create_tf_example(row):\n",
        "    # Mengonversi gambar menjadi bytes\n",
        "    image_path = os.path.join(image_dir, row['filename'])\n",
        "\n",
        "    with tf.io.gfile.GFile(image_path, 'rb') as fid:\n",
        "        encoded_image = fid.read()\n",
        "\n",
        "    feature = {\n",
        "        'image/encoded': _bytes_feature(encoded_image),\n",
        "        'image/format': _bytes_feature(b'jpeg'),\n",
        "        'image/object/bbox/xmin': _float_feature(row['xmin']),\n",
        "        'image/object/bbox/xmax': _float_feature(row['xmax']),\n",
        "        'image/object/bbox/ymin': _float_feature(row['ymin']),\n",
        "        'image/object/bbox/ymax': _float_feature(row['ymax']),\n",
        "        'image/object/class/label': _int64_feature(class_text_to_int(row['class'])),\n",
        "    }\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return tf_example\n",
        "\n",
        "csv_file_path = '/content/test_resized_640.csv'\n",
        "\n",
        "# Membaca file CSV\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Gantilah dengan path untuk menyimpan file TFRecord\n",
        "output_tfrecord_path = '/content/test.tfrecord'\n",
        "\n",
        "# Menulis data ke dalam file TFRecord\n",
        "with tf.io.TFRecordWriter(output_tfrecord_path) as writer:\n",
        "    for index, row in df.iterrows():\n",
        "        tf_example = create_tf_example(row)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "print(f\"TFRecord file created at: {output_tfrecord_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ONmgY92NLL6",
        "outputId": "b1bbdca5-9e1f-4ac8-e283-faf1811578a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecord file created at: /content/test.tfrecord\n"
          ]
        }
      ]
    }
  ]
}